I've found a lot of good resources motivating MCMC methods, and describing the relevant algorithms, chiefly Gibbs sampling, Metropolis sampling, and Hybrid Monte-Carlo. When attempting to actually apply these tools, I've come across a few tricks and concepts that weren't clearly signposted in the literature. (The specific problems that required the use of MCMC were [survival curves](https://hilbert-spaess.github.io/STATS-survival-curves/) problems and full parameter inference of neural network models).

## Motivation and intuition

Assume we have a distribution over a high-dimensional vector of parameters $\theta$. We want to get some decent independent samples of $\theta$.

## Chains-within-chains

## ARS

## Dimension-jumping

## Checking everything's working
